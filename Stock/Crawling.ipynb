{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import ssl\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def GetKospi200():     \n",
    "    stockDic = dict()\n",
    "    lastPageNum = 0\n",
    "\n",
    "    # 마지막 페이지 찾기\n",
    "    base_url = \"http://finance.naver.com/sise/entryJongmok.nhn?&page=\"\n",
    "    target_url = base_url + str(1)\n",
    "    context = ssl._create_unverified_context()\n",
    "    # soup = BeautifulSoup(urllib.request.urlopen(target_url).read(), \"lxml\")\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(target_url, context=context).read().decode('euc-kr', 'ignore'), \"lxml\")\n",
    "    for item in soup.find_all('td'):\n",
    "        if item.has_attr('class') and 'pgRR' in item['class']:\n",
    "            lastPageNum = int(str(item.a['href']).replace('/sise/entryJongmok.nhn?&page=', ''))\n",
    "\n",
    "    for i in range(1, lastPageNum+1):\n",
    "        target_url = base_url + str(i)\n",
    "        # soup = BeautifulSoup(urllib.request.urlopen(target_url).read(), \"lxml\")\n",
    "        soup = BeautifulSoup(urllib.request.urlopen(target_url, context=context).read().decode('euc-kr', 'ignore'), \"lxml\")\n",
    "        postNoList = soup.find_all('a')\n",
    "\n",
    "\n",
    "        # 종목코드와 종목명 담기\n",
    "        for item in postNoList:\n",
    "            if item.has_attr('target') and '_parent' in item['target'] and item.has_attr('href'):\n",
    "                if str(item['href']).startswith('/item/main.nhn?code='):\n",
    "                    stockDic[str(item['href']).replace('/item/main.nhn?code=', '')] = item.text\n",
    "    return stockDic\n",
    "\n",
    "def getKospiIndex():\n",
    "    outFileName = \"data/kospi.csv\"\n",
    "    i = 1\n",
    "    last_date = \"\"\n",
    "    while True:\n",
    "        if os.path.exists(outFileName):\n",
    "            df = pd.read_csv(outFileName, names=[\"date\", \"index\", \"diff\", \"diff_ratio\", \"amount\", \"tot_price\"])\n",
    "\n",
    "        with open(outFileName, mode = 'a') as f: \n",
    "            url = \"https://finance.naver.com/sise/sise_index_day.nhn?code=KOSPI&page=\" + str(i)\n",
    "            print(url)            \n",
    "            table = pd.read_html(url)\n",
    "            table = table[0].dropna()           \n",
    "            if last_date == table[\"날짜\"].max():\n",
    "                f.close()                \n",
    "                break\n",
    "            else:\n",
    "                last_date = table[\"날짜\"].max()\n",
    "    #             table.to_csv(f, header=False, index=False)\n",
    "                if len(table[~table[\"날짜\"].isin(df[\"date\"].tolist())]) == 0:\n",
    "                    print(\"종료\")\n",
    "                    break\n",
    "                table[~table[\"날짜\"].isin(df[\"date\"].tolist())].to_csv(f, header=False, index=False)\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def GetPriceData(item):    \n",
    "    \n",
    "    # print(item)\n",
    "    code = item[0]\n",
    "    name = item[1]\n",
    "                \n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "    url = \"http://finance.naver.com/item/sise_day.nhn?code=\" + code\n",
    "    page = \"&page=\"\n",
    "    idx = 1\n",
    "\n",
    "    outputFileName = directory + code + '.csv'\n",
    "    \n",
    "    df = pd.read_csv(outputFileName)\n",
    "    bFlag = True\n",
    "    with open(outputFileName, 'a') as f:\n",
    "        while bFlag == True:\n",
    "            fullAddr = url + page + str(idx)\n",
    "\n",
    "            source_code = requests.get(fullAddr, headers = headers)\n",
    "            if source_code is None:\n",
    "                bFlag = False\n",
    "                return\n",
    "\n",
    "            soup = BeautifulSoup(source_code.text,\"lxml\")\n",
    "            if soup.find('td', class_='on').find('a').text != str(idx):\n",
    "                bFlag = False\n",
    "                return\n",
    "\n",
    "            for tr in filter(lambda x:x.get(\"onmouseout\") is not None, soup.find_all(\"tr\")):\n",
    "                if tr.find(\"span\",class_ = \"tah p10 gray03\") is None:\n",
    "                    # 가격데이터가 없으면 False로 빠져나옴\n",
    "                    bFlag = False\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    tDate = tr.find(\"span\",class_ = \"tah p10 gray03\").text\n",
    "                    cPrice = tr.find_all(\"span\",class_ = \"tah p11\")\n",
    "                    sIdx = 1\n",
    "\n",
    "                    if len(cPrice) != 5 :\n",
    "                        sIdx = 2\n",
    "\n",
    "                    dt = tDate.replace(\".\" ,\"-\")\n",
    "                    if (df[\"date\"] == dt).sum() > 0:                        \n",
    "#                         print(dt, \" 가격존재 \")\n",
    "                        f.close()\n",
    "                        bFlag = False\n",
    "                        break\n",
    "\n",
    "                    pClose = float(cPrice[0].text.replace(\",\" ,\"\"))\n",
    "                    pStart = float(cPrice[sIdx].text.replace(\",\" ,\"\"))\n",
    "                    sIdx += 1\n",
    "                    pMax   = float(cPrice[sIdx].text.replace(\",\" ,\"\"))\n",
    "                    sIdx += 1\n",
    "                    pMin   = float(cPrice[sIdx].text.replace(\",\" ,\"\"))\n",
    "                    sIdx += 1\n",
    "                    amount = float(cPrice[sIdx].text.replace(\",\" ,\"\"))\n",
    "\n",
    "                    stockList = []\n",
    "                    stockList.append(dt)\n",
    "                    stockList.append(code)\n",
    "                    stockList.append(pStart)\n",
    "                    stockList.append(pClose)\n",
    "                    stockList.append(pMin)\n",
    "                    stockList.append(pMax)\n",
    "                    stockList.append(amount)                \n",
    "                    f.writelines(\",\".join([str(x) for x in stockList]) + '\\n') \n",
    "            idx += 1\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests \n",
    "import os\n",
    "\n",
    "def getFinanceInfo(code=None):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "    # 투자정보\n",
    "    # totalCnt = []  # 상장주식수\n",
    "    # forignerHaveLimit = []  # 외국인한도보유주식수\n",
    "    # forignerHaveCnt = []  # 외국인보유주식수\n",
    "    # max52week = []  # 52주 최고\n",
    "    # min52week = []  # 52주 최저\n",
    "    # per = []\n",
    "    # eps = []\n",
    "    # per_eps_date = []\n",
    "    # estimate_per = []  # 추정 PER\n",
    "    # estimate_eps = []  # 추정 EPS\n",
    "    # pbr = []\n",
    "    # bps = []\n",
    "    # pbr_bps_date = []\n",
    "    # dvr = []  # 배당수익\n",
    "    financeFileName = 'data/stock_finance_data.csv'\n",
    "    cols = [\"date\",\n",
    "             \"code\", \n",
    "             \"totalCnt\", \n",
    "             \"forignerHaveLimit\",\n",
    "             \"forignerHaveCnt\",\n",
    "             \"min52week\",\n",
    "             \"max52week\",\n",
    "             \"per\",\n",
    "             \"eps\",\n",
    "             \"per_eps_date\",\n",
    "             \"estimate_per\",\n",
    "             \"estimate_eps\",\n",
    "             \"pbr\",\n",
    "             \"bps\",\n",
    "             \"pbr_bps_date\",\n",
    "             \"dvr\"]\n",
    "    if os.path.exists(financeFileName):\n",
    "        df = pd.read_csv(financeFileName, header=None, names=cols, dtype=str)\n",
    "\n",
    "    url = \"https://finance.naver.com/item/main.nhn?code=\" + code\n",
    "    source_code = requests.get(url, headers = headers)\n",
    "    if source_code is None:\n",
    "        pass\n",
    "    else:\n",
    "        soup = BeautifulSoup(source_code.text, \"lxml\")\n",
    "        date = soup.find(id='time').find_all('em')[0].text\n",
    "        \n",
    "        # 이미 존재하는 데이터면 skip\n",
    "        if ((df[\"date\"] == date) & (df[\"code\"] == code)).sum() > 0:\n",
    "            return\n",
    "        \n",
    "        totalCnt = soup.find(id='tab_con1').find_all('em')[2].text.replace(',', '').replace('N/A', '0')\n",
    "        forignerHaveLimit = soup.find(id='tab_con1').find_all('em')[5].text.replace(',', '').replace('N/A', '0')\n",
    "        forignerHaveCnt = soup.find(id='tab_con1').find_all('em')[6].text.replace(',', '').replace('N/A', '0')\n",
    "        max52week = soup.find(id='tab_con1').find_all('em')[10].text.replace(',', '').replace('N/A', '0')\n",
    "        min52week = soup.find(id='tab_con1').find_all('em')[11].text.replace(',', '').replace('N/A', '0')\n",
    "        per = soup.find(id='tab_con1').find_all('em')[12].text.replace(',', '').replace('N/A', '0')\n",
    "        eps = soup.find(id='tab_con1').find_all('em')[13].text.replace(',', '').replace('N/A', '0')\n",
    "        per_eps_date = soup.find(id='tab_con1').find_all('span', class_='date')[0].text.replace('(','').replace(')','') if len(soup.find(id='tab_con1').find_all('span', class_='date')) > 0 else \"\"\n",
    "        estimate_per = soup.find(id='tab_con1').find_all('em')[14].text.replace(',', '').replace('N/A', '0')\n",
    "        estimate_eps = soup.find(id='tab_con1').find_all('em')[15].text.replace(',', '').replace('N/A', '0')\n",
    "        pbr = soup.find(id='tab_con1').find_all('em')[16].text.replace(',', '').replace('N/A', '0')\n",
    "        bps = soup.find(id='tab_con1').find_all('em')[17].text.replace(',', '').replace('N/A', '0')\n",
    "        pbr_bps_date = soup.find(id='tab_con1').find_all('span', class_='date')[1].text.replace('(','').replace(')','') if len(soup.find(id='tab_con1').find_all('span', class_='date')) > 1 else \"\"\n",
    "        dvr = soup.find(id='tab_con1').find_all('em')[18].text.replace(',', '').replace('N/A', '0')                \n",
    "        \n",
    "        financeList = []\n",
    "        financeList.append(date)\n",
    "        financeList.append(str(code))\n",
    "        financeList.append(totalCnt)\n",
    "        financeList.append(forignerHaveLimit)\n",
    "        financeList.append(forignerHaveCnt)        \n",
    "        financeList.append(max52week)\n",
    "        financeList.append(min52week)\n",
    "        financeList.append(per)\n",
    "        financeList.append(eps)\n",
    "        financeList.append(per_eps_date)\n",
    "        financeList.append(estimate_per)\n",
    "        financeList.append(estimate_eps)\n",
    "        financeList.append(pbr)\n",
    "        financeList.append(bps)\n",
    "        financeList.append(pbr_bps_date)\n",
    "        financeList.append(dvr)    \n",
    "        \n",
    "        with open(financeFileName, 'a') as f:\n",
    "            f.writelines(\",\".join([str(x) for x in financeList]) + '\\n') \n",
    "        f.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests \n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def crawling():\n",
    "#     file_list = os.listdir('data')\n",
    "#     exist_list = []\n",
    "#     for file in file_list:\n",
    "#         if 'finance' not in file and '.csv' in file:\n",
    "#             exist_list.append(file)\n",
    "\n",
    "    directory = 'data/'\n",
    "    stockDict = GetKospi200()\n",
    "    # stockDict = {'192400' : '쿠쿠홀딩스'}\n",
    "\n",
    "    kospiListFile = \"kospi_list.csv\"\n",
    "    bFlag = False\n",
    "    if os.path.exists(directory+kospiListFile):\n",
    "        bFlag = bool((pd.read_csv(directory+kospiListFile, \n",
    "                                  names=[\"date\", \"code\", \"name\"], \n",
    "                                  header=None)[\"date\"] == datetime.today().strftime(\"%Y-%m-%d\")).sum())\n",
    "    \n",
    "    with open(directory + \"/\" + kospiListFile, 'a') as f:\n",
    "        for key in stockDict.keys():\n",
    "            print(stockDict[key], key)\n",
    "            if not bFlag:        \n",
    "                l = []\n",
    "                l.append(datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "                l.append(key)\n",
    "                l.append(stockDict[key])\n",
    "                f.writelines(\",\".join([str(x) for x in l]) + '\\n') \n",
    "\n",
    "            #if str(key) not in exist_list:\n",
    "            target = (key, stockDict[key])  \n",
    "            GetPriceData(target) \n",
    "            getFinanceInfo(key)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자 005930\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "SK하이닉스 000660\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "NAVER 035420\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "LG화학 051910\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "현대차 005380\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "삼성바이오로직스 207940\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "삼성SDI 006400\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "카카오 035720\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "셀트리온 068270\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "기아차 000270\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "현대모비스 012330\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "POSCO 005490\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "SK이노베이션 096770\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "LG전자 066570\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "LG생활건강 051900\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "삼성물산 028260\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "엔씨소프트 036570\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "SK텔레콤 017670\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "SK 034730\n",
      "GetPriceData\n",
      "getFinanceInfo\n",
      "KB금융 105560\n",
      "GetPriceData\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/105560.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-0e7c3126ec65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-06093d8d7cfe>\u001b[0m in \u001b[0;36mcrawling\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstockDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GetPriceData\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mGetPriceData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"getFinanceInfo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mgetFinanceInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-56c192b9aa70>\u001b[0m in \u001b[0;36mGetPriceData\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutputFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputFileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mbFlag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputFileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/105560.csv'"
     ]
    }
   ],
   "source": [
    "crawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정날짜 데이터 제거 및 reset_index\n",
    "import pandas as pd\n",
    "def removeData(date = \"2021-02-26\"):\n",
    "    directory = 'data'\n",
    "    file_list = os.listdir(directory)\n",
    "\n",
    "    for file in file_list:\n",
    "        if 'finance' not in file and '.csv' in file and 'kospi' not in file:   \n",
    "            print(file)\n",
    "            df = pd.read_csv(directory + \"/\" + file)\n",
    "            if \"Unnamed: 0\" in df.columns:\n",
    "                df = pd.read_csv(directory + \"/\" + file).drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "            if (df[\"date\"] == date).sum() > 0:\n",
    "                df = df.drop(df[df[\"date\"] == date].index)\n",
    "                df.to_csv(directory + \"/\" + file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정날짜 데이터 제거 및 reset_index\n",
    "import pandas as pd\n",
    "def resetIndex():\n",
    "#     cols = [\"date\", \"code\", \"start\", \"close\", \"min\", \"max\", \"amount\"]\n",
    "    directory = 'data'\n",
    "    file_list = os.listdir(directory)\n",
    "\n",
    "    for file in file_list:\n",
    "        if 'finance' not in file and '.csv' in file and 'kospi' not in file:     \n",
    "            print(file)\n",
    "            df = pd.read_csv(directory + \"/\" + file)\n",
    "            if \"Unnamed: 0\" in df.columns:\n",
    "                df = pd.read_csv(directory + \"/\" + file).drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "            df = df.sort_values(by=\"date\").reset_index().drop(\"index\", axis=1)\n",
    "            df.to_csv(directory + \"/\" + file, index=False)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
